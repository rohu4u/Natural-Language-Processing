{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ecfb1de-8914-4f21-9725-f99040b963ad",
   "metadata": {
    "id": "0ecfb1de-8914-4f21-9725-f99040b963ad"
   },
   "source": [
    "# Import Dataset and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8361b3-d24c-45fd-a5fc-23115af3ace3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2e8361b3-d24c-45fd-a5fc-23115af3ace3",
    "outputId": "02b1b149-da81-4b12-a015-4a039fdd74b7"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import string # special operations on strings\n",
    "import spacy # language models\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import spacy.cli\n",
    "spacy.cli.download(\"en_core_web_md\")\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # Hides warning\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\",category=UserWarning)\n",
    "sns.set_style(\"whitegrid\") # Plotting style\n",
    "%matplotlib inline # Plots show up in notebook\n",
    "np.random.seed(7) # seeding random number generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec81e351-18b5-4969-96b1-1bfc30f69ca1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "ec81e351-18b5-4969-96b1-1bfc30f69ca1",
    "outputId": "67a1e77f-7013-476f-ef83-d5e800f13809"
   },
   "outputs": [],
   "source": [
    "reviews = pd.read_csv(\"Product_details.csv\")\n",
    "reviews['Product_Description'] = reviews['Product_Description'].str.lower()\n",
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94716fce-9d42-49ad-839f-745bfdae6ff4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "94716fce-9d42-49ad-839f-745bfdae6ff4",
    "outputId": "32c44bda-b024-4b31-8969-700db393bf8f"
   },
   "outputs": [],
   "source": [
    "reviews1 = reviews['Product_Description']\n",
    "reviews1 = reviews1.astype(str)\n",
    "reviews1 = reviews1[reviews1.str.contains('nan')==False]\n",
    "reviews1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482029bd-92b6-4198-bfe1-cfd32ea0dec3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "482029bd-92b6-4198-bfe1-cfd32ea0dec3",
    "outputId": "078383a9-dfcb-448b-80c4-2b039b97a0f8"
   },
   "outputs": [],
   "source": [
    "reviews1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1TC-x_JDd19e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "1TC-x_JDd19e",
    "outputId": "03036919-ef09-43fe-e0d7-d68560f09f13"
   },
   "outputs": [],
   "source": [
    "reviews = reviews.drop_duplicates(subset=\"Product_Description\",keep=False)\n",
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "R1lSSWsFpyGI",
   "metadata": {
    "id": "R1lSSWsFpyGI"
   },
   "outputs": [],
   "source": [
    "reviews2 = [y.strip() for y in reviews.Product_Description]\n",
    "reviews2 = [x for x in reviews2 if x]\n",
    "text = ' '.join(reviews2)\n",
    "no_punc_text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "no_punc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a933017",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend([\"mention\",\"sxsw\",\"SXSW\",\"RT\",\"link\",\"Austin\",\"amp\",\"via\",\"be\",\"4\",\"sxswi\",\"re\",\"if\",\"3\",\"5\",\"a\",\"u\",\"SxSW\",\"s\",\"re\",\"m\",\"SXSWi\",\"\\x89ÛÏmention\",\"pm\",\"fb\",\"ll\",\"sxsw\\x89Û\\x9d\",\"2\",\"qagb\",\"Sxsw\",\"\\x89ûïmention\",\"I\"])\n",
    "\n",
    "reviews = [word for word in no_punc_text if not word in stop_words]\n",
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbe31fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot word cloud\n",
    "\n",
    "def plot_cloud(wordcloud):\n",
    "    \n",
    "    # Set figure size\n",
    "    plt.figure(figsize=(60, 40))\n",
    "\n",
    "    # Display image\n",
    "    plt.imshow(wordcloud) \n",
    "    \n",
    "    # No axis details\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "R8VxB-PxovFx",
   "metadata": {
    "id": "R8VxB-PxovFx"
   },
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width = 3000,\n",
    "                      height = 1500,\n",
    "                      background_color='black').generate(no_punc_text)\n",
    "# Plot\n",
    "plot_cloud(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb054ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv(\"Product_details.csv\")\n",
    "reviews['Product_Description'] = reviews['Product_Description'].str.lower()\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ee8OdPxDvGBG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ee8OdPxDvGBG",
    "outputId": "df6e1a46-88dd-4265-86d6-e964fc73a287"
   },
   "outputs": [],
   "source": [
    "reviews.Product_Type.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xG9xyD_NUBuv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xG9xyD_NUBuv",
    "outputId": "3448e040-08de-4ad3-81e6-5a67a28b33e7"
   },
   "outputs": [],
   "source": [
    "reviews.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3MCe6cIqvF-K",
   "metadata": {
    "id": "3MCe6cIqvF-K"
   },
   "outputs": [],
   "source": [
    "df0 = reviews[reviews['Product_Type'] == 0]\n",
    "df1 = reviews[reviews['Product_Type'] == 1]\n",
    "df2 = reviews[reviews['Product_Type'] == 2]\n",
    "df3 = reviews[reviews['Product_Type'] == 3]\n",
    "df4 = reviews[reviews['Product_Type'] == 4]\n",
    "df5 = reviews[reviews['Product_Type'] == 5]\n",
    "df6 = reviews[reviews['Product_Type'] == 6]\n",
    "df7 = reviews[reviews['Product_Type'] == 7]\n",
    "df8 = reviews[reviews['Product_Type'] == 8]\n",
    "df9 = reviews[reviews['Product_Type'] == 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Z_UUGXKcvF7q",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 958
    },
    "id": "Z_UUGXKcvF7q",
    "outputId": "e5b08d6f-98c8-480a-d0e1-1db855eb31bf"
   },
   "outputs": [],
   "source": [
    "temp = [y.strip() for y in df0.Product_Description]\n",
    "temp = [x for x in temp if x]\n",
    "text = ' '.join(temp)\n",
    "no_punc_text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "wordcloud = WordCloud(width = 3000,\n",
    "                      height = 1500,\n",
    "                      background_color='black').generate(text)\n",
    "# Plot\n",
    "plot_cloud(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qd-QhmRDvF4h",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 725
    },
    "id": "qd-QhmRDvF4h",
    "outputId": "e60947ab-75a9-4f64-990a-5f8678844145"
   },
   "outputs": [],
   "source": [
    "temp = [y.strip() for y in df1.Product_Description]\n",
    "temp = [x for x in temp if x]\n",
    "text = ' '.join(temp)\n",
    "no_punc_text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "wordcloud = WordCloud(width = 3000,\n",
    "                      height = 1500,\n",
    "                      background_color='black').generate(text)\n",
    "# Plot\n",
    "plot_cloud(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TDfw8gTuvF1q",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 725
    },
    "id": "TDfw8gTuvF1q",
    "outputId": "84b54aa4-c608-4d50-ec7e-d76bf2d3dc17"
   },
   "outputs": [],
   "source": [
    "temp = [y.strip() for y in df2.Product_Description]\n",
    "temp = [x for x in temp if x]\n",
    "text = ' '.join(temp)\n",
    "no_punc_text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "wordcloud = WordCloud(width = 3000,\n",
    "                      height = 1500,\n",
    "                      background_color='black').generate(text)\n",
    "# Plot\n",
    "plot_cloud(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5aaPvsnvFy0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 725
    },
    "id": "e5aaPvsnvFy0",
    "outputId": "4e12e32c-7697-448e-dc95-a6efdfbbe9ab"
   },
   "outputs": [],
   "source": [
    "temp = [y.strip() for y in df3.Product_Description]\n",
    "temp = [x for x in temp if x]\n",
    "text = ' '.join(temp)\n",
    "no_punc_text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "wordcloud = WordCloud(width = 3000,\n",
    "                      height = 1500,\n",
    "                      background_color='black').generate(text)\n",
    "# Plot\n",
    "plot_cloud(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "B0HEwgUpvFvs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 725
    },
    "id": "B0HEwgUpvFvs",
    "outputId": "9e3c8a63-1bce-4906-8730-2268d22300f5"
   },
   "outputs": [],
   "source": [
    "temp = [y.strip() for y in df4.Product_Description]\n",
    "temp = [x for x in temp if x]\n",
    "text = ' '.join(temp)\n",
    "no_punc_text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "wordcloud = WordCloud(width = 3000,\n",
    "                      height = 1500,\n",
    "                      background_color='black').generate(text)\n",
    "# Plot\n",
    "plot_cloud(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1GZFRF_HvFs3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 725
    },
    "id": "1GZFRF_HvFs3",
    "outputId": "fcd537bd-c200-4c2b-f02c-9e9b6ff8330d"
   },
   "outputs": [],
   "source": [
    "temp = [y.strip() for y in df5.Product_Description]\n",
    "temp = [x for x in temp if x]\n",
    "text = ' '.join(temp)\n",
    "no_punc_text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "wordcloud = WordCloud(width = 3000,\n",
    "                      height = 1500,\n",
    "                      background_color='black').generate(text)\n",
    "# Plot\n",
    "plot_cloud(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SAvMWuxgvFp1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 725
    },
    "id": "SAvMWuxgvFp1",
    "outputId": "d1dac565-43db-46e0-a4a8-00a3ec0e6fab"
   },
   "outputs": [],
   "source": [
    "temp = [y.strip() for y in df6.Product_Description]\n",
    "temp = [x for x in temp if x]\n",
    "text = ' '.join(temp)\n",
    "no_punc_text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "wordcloud = WordCloud(width = 3000,\n",
    "                      height = 1500,\n",
    "                      background_color='black').generate(text)\n",
    "# Plot\n",
    "plot_cloud(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-qZdsWaMvFm6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 725
    },
    "id": "-qZdsWaMvFm6",
    "outputId": "1ad9bdc5-ab9f-4a01-c7ab-051fa6b0127d"
   },
   "outputs": [],
   "source": [
    "temp = [y.strip() for y in df7.Product_Description]\n",
    "temp = [x for x in temp if x]\n",
    "text = ' '.join(temp)\n",
    "no_punc_text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "wordcloud = WordCloud(width = 3000,\n",
    "                      height = 1500,\n",
    "                      background_color='black').generate(text)\n",
    "# Plot\n",
    "plot_cloud(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86GXtUjFvFj4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 725
    },
    "id": "86GXtUjFvFj4",
    "outputId": "f2ac7b72-9690-47a0-dd14-191409cd9b93"
   },
   "outputs": [],
   "source": [
    "temp = [y.strip() for y in df8.Product_Description]\n",
    "temp = [x for x in temp if x]\n",
    "text = ' '.join(temp)\n",
    "no_punc_text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "wordcloud = WordCloud(width = 3000,\n",
    "                      height = 1500,\n",
    "                      background_color='black').generate(text)\n",
    "# Plot\n",
    "plot_cloud(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nyfJJ46WvFgp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 725
    },
    "id": "nyfJJ46WvFgp",
    "outputId": "22c0551e-8174-47fa-cd33-15a13fc3229d"
   },
   "outputs": [],
   "source": [
    "temp = [y.strip() for y in df9.Product_Description]\n",
    "temp = [x for x in temp if x]\n",
    "text = ' '.join(temp)\n",
    "no_punc_text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "wordcloud = WordCloud(width = 3000,\n",
    "                      height = 1500,\n",
    "                      background_color='black').generate(text)\n",
    "# Plot\n",
    "plot_cloud(wordcloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5636b6d1-a827-4728-8a18-55dc493d4933",
   "metadata": {
    "id": "5636b6d1-a827-4728-8a18-55dc493d4933"
   },
   "source": [
    "## Sentimental Analysis using Afinn(Standard file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f792d1b-6f9b-4087-8435-e88600c43a8a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "id": "1f792d1b-6f9b-4087-8435-e88600c43a8a",
    "outputId": "e38696d8-c5b1-4e4c-e7c6-d78d8346f737"
   },
   "outputs": [],
   "source": [
    "#Sentiment analysis\n",
    "afinn = pd.read_csv('afinn.csv', encoding = 'latin1')\n",
    "afinn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5E56OgfY2ujB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5E56OgfY2ujB",
    "outputId": "860282ba-03eb-4694-ed1d-00237283d39b"
   },
   "outputs": [],
   "source": [
    "reviews.Product_Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e948bcda-b1c5-44f6-975e-b9cdd3d009fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e948bcda-b1c5-44f6-975e-b9cdd3d009fd",
    "outputId": "1914ffa1-a26f-4ef3-f36c-8a6a02e96e44"
   },
   "outputs": [],
   "source": [
    "sentences = reviews.Product_Description.tolist()\n",
    "\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c8ed15-441a-4454-beef-e597e7970412",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "68c8ed15-441a-4454-beef-e597e7970412",
    "outputId": "c382cee7-da2c-4883-fedd-f12a88158048"
   },
   "outputs": [],
   "source": [
    "sent_df = pd.DataFrame(sentences, columns=['sentence'])\n",
    "sent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e51074-0959-4764-9e5a-a9591348b4b5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "81e51074-0959-4764-9e5a-a9591348b4b5",
    "outputId": "2c54af7c-6ebe-48e0-db0f-eed0b08fe31a"
   },
   "outputs": [],
   "source": [
    "affinity_scores = afinn.set_index('word')['value'].to_dict()\n",
    "affinity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dDCaHbh0xiVl",
   "metadata": {
    "id": "dDCaHbh0xiVl"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01db687-584e-4ab3-8ef5-5216325a6880",
   "metadata": {
    "id": "d01db687-584e-4ab3-8ef5-5216325a6880"
   },
   "outputs": [],
   "source": [
    "#Custom function :score each word in a sentence in lemmatised form, \n",
    "#but calculate the score for the whole original sentence.\n",
    "sentiment_lexicon = affinity_scores\n",
    "\n",
    "def calculate_sentiment(text: str = None):\n",
    "    sent_score = 0\n",
    "    if text:\n",
    "        sentence = nlp(text)\n",
    "        #print(sentence)\n",
    "        for word in sentence:\n",
    "            sent_score += sentiment_lexicon.get(word.lemma_, 0) #return 0 if key not found\n",
    "    return sent_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967074af-9b57-4894-8c59-f463709b44d4",
   "metadata": {
    "id": "967074af-9b57-4894-8c59-f463709b44d4"
   },
   "outputs": [],
   "source": [
    "sent_df['sentiment_value'] = sent_df['sentence'].apply(calculate_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae05b5db-af6d-47ad-9b76-c4715a3b6844",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "ae05b5db-af6d-47ad-9b76-c4715a3b6844",
    "outputId": "7cc60a38-85b5-4bff-ec85-573fdd50d850"
   },
   "outputs": [],
   "source": [
    "sent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vHA5zZxfzxP0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "id": "vHA5zZxfzxP0",
    "outputId": "835c930a-cced-4e7a-fc99-4949c9858ea5"
   },
   "outputs": [],
   "source": [
    "#Sentiment analysis\n",
    "afinn = pd.read_csv('afinn.csv', encoding = 'latin1')\n",
    "sentences = reviews.Product_Description.tolist()\n",
    "sent_df = pd.DataFrame(sentences, columns=['sentence'])\n",
    "affinity_scores = afinn.set_index('word')['value'].to_dict()\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "sentiment_lexicon = affinity_scores\n",
    "\n",
    "def calculate_sentiment(text: str = None):\n",
    "    sent_score = 0\n",
    "    if text:\n",
    "        sentence = nlp(text)\n",
    "        for word in sentence:\n",
    "            sent_score += sentiment_lexicon.get(word.lemma_, 0) #return 0 if key not found\n",
    "    return sent_score\n",
    "\n",
    "sent_df['sentiment_value'] = sent_df['sentence'].apply(calculate_sentiment)\n",
    "sent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GUlIESSr__NB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "GUlIESSr__NB",
    "outputId": "e625133a-4350-404f-f242-633355890fe4"
   },
   "outputs": [],
   "source": [
    "reviews['Afinn Score'] = sent_df['sentiment_value']\n",
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QqBd_1gy__Jk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "QqBd_1gy__Jk",
    "outputId": "b2bd0c6b-0ab4-4a73-c7d6-67633e10d6b4"
   },
   "outputs": [],
   "source": [
    "sns.stripplot(data = reviews,x = reviews['Afinn Score'],y = reviews['Sentiment'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99da0d98-a5b5-42e5-a5a2-f9c4ab36be64",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "99da0d98-a5b5-42e5-a5a2-f9c4ab36be64",
    "outputId": "b0561709-2ef3-4ad6-f676-b24a5650270c"
   },
   "outputs": [],
   "source": [
    "# how many words are in the sentence?\n",
    "sent_df['word_count'] = sent_df['sentence'].str.split().apply(len)\n",
    "#sent_df['word_count'].head(10)\n",
    "sent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d58002f-4ae0-47c8-8d0c-6faecd7d5af6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "3d58002f-4ae0-47c8-8d0c-6faecd7d5af6",
    "outputId": "afbda921-ea0d-4956-eab7-c91d9d78e713"
   },
   "outputs": [],
   "source": [
    "sent_df.sort_values(by='sentiment_value').tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0572ed-79df-4c3d-b4c7-7a1ed406772a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "6b0572ed-79df-4c3d-b4c7-7a1ed406772a",
    "outputId": "e0b077d7-6845-4cbd-cb2c-200efbaf6b94"
   },
   "outputs": [],
   "source": [
    "sent_df.sort_values(by='sentiment_value').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46744eb2-41d3-461c-b9af-a71ee67c5ecb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "46744eb2-41d3-461c-b9af-a71ee67c5ecb",
    "outputId": "7c350820-b85b-4471-ae2e-77521e5df56d"
   },
   "outputs": [],
   "source": [
    "# Sentiment score of the whole review\n",
    "sent_df['sentiment_value'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7d55c2-f9a7-4028-9fec-e1688e065d1a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 352
    },
    "id": "6c7d55c2-f9a7-4028-9fec-e1688e065d1a",
    "outputId": "4e62fc8c-835f-40c1-c0b2-753827e6aef3"
   },
   "outputs": [],
   "source": [
    "sns.distplot(sent_df['sentiment_value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c39c661-c3a5-4120-bb25-57fdf24054b4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 531
    },
    "id": "4c39c661-c3a5-4120-bb25-57fdf24054b4",
    "outputId": "caee7ac1-f081-4887-cea6-8df399386262"
   },
   "outputs": [],
   "source": [
    "sent_df.plot.scatter(x='word_count',\n",
    "                     y='sentiment_value',\n",
    "                     figsize=(8,8),\n",
    "                     title='Sentence sentiment value to sentence word count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Dk710mZOLlTJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Dk710mZOLlTJ",
    "outputId": "609a2991-32d8-4f5b-a53b-2f7958176e75"
   },
   "outputs": [],
   "source": [
    "doc_block = nlp(text)\n",
    "spacy.displacy.render(doc_block, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chPRqEbYL1rk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "chPRqEbYL1rk",
    "outputId": "d597a483-a7ec-4f9a-eac6-d544b24bcab2"
   },
   "outputs": [],
   "source": [
    "for token in doc_block[:]:\n",
    "    print(token, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kD8k9TkXL3qT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kD8k9TkXL3qT",
    "outputId": "d0f6aeec-318a-4894-f38a-9bc6196395e3"
   },
   "outputs": [],
   "source": [
    "nouns_verbs = [token.text for token in doc_block if token.pos_ in ('NOUN', 'VERB')]\n",
    "\n",
    "nouns_verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Nah6kU-LM_bw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "Nah6kU-LM_bw",
    "outputId": "cce8d3b2-ca42-4136-e901-03e5f2956b90"
   },
   "outputs": [],
   "source": [
    "adj = [token.text for token in doc_block if token.pos_ in ('ADJ')]\n",
    "adj1 = ' '.join([str(elem) for elem in adj])\n",
    "adj1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c493e32",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 725
    },
    "id": "6c493e32",
    "outputId": "e8e2416f-6127-4d64-89be-f7cb37554577"
   },
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width = 3000,\n",
    "                      height = 1500,\n",
    "                      background_color='black').generate(adj1)\n",
    "# Plot\n",
    "plot_cloud(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Dwd_w2W-KyKu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Dwd_w2W-KyKu",
    "outputId": "03073715-3036-47da-f457-5985d78c154f"
   },
   "outputs": [],
   "source": [
    "temp = [y.strip() for y in df0.Product_Description]\n",
    "temp = [x for x in temp if x]\n",
    "text = ' '.join(temp)\n",
    "doc_block = nlp(text)\n",
    "spacy.displacy.render(doc_block, style='ent', jupyter=True)\n",
    "for token in doc_block[:]:\n",
    "    print(token, token.pos_)\n",
    "nouns_verbs = [token.text for token in doc_block if token.pos_ in ('NOUN', 'VERB')]\n",
    "\n",
    "adj = [token.text for token in doc_block if token.pos_ in ('ADJ')]\n",
    "adj1 = ' '.join([str(elem) for elem in adj])\n",
    "wordcloud = WordCloud(width = 3000,\n",
    "                      height = 1500,\n",
    "                      background_color='black').generate(adj1)\n",
    "# Plot\n",
    "plot_cloud(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WaQ8kDmuKyHu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WaQ8kDmuKyHu",
    "outputId": "4819ff5c-3d61-4bcf-97b4-363d63e7f8aa"
   },
   "outputs": [],
   "source": [
    "temp = [y.strip() for y in df1.Product_Description]\n",
    "temp = [x for x in temp if x]\n",
    "text = ' '.join(temp)\n",
    "doc_block = nlp(text)\n",
    "spacy.displacy.render(doc_block, style='ent', jupyter=True)\n",
    "for token in doc_block[:]:\n",
    "    print(token, token.pos_)\n",
    "nouns_verbs = [token.text for token in doc_block if token.pos_ in ('NOUN', 'VERB')]\n",
    "\n",
    "adj = [token.text for token in doc_block if token.pos_ in ('ADJ')]\n",
    "adj1 = ' '.join([str(elem) for elem in adj])\n",
    "wordcloud = WordCloud(width = 3000,\n",
    "                      height = 1500,\n",
    "                      background_color='black').generate(adj1)\n",
    "# Plot\n",
    "plot_cloud(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tFO9-F3sKyEr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "tFO9-F3sKyEr",
    "outputId": "0ba60377-9c3b-44d6-9280-471fc96cdc51"
   },
   "outputs": [],
   "source": [
    "temp = [y.strip() for y in df2.Product_Description]\n",
    "temp = [x for x in temp if x]\n",
    "text = ' '.join(temp)\n",
    "doc_block = nlp(text)\n",
    "spacy.displacy.render(doc_block, style='ent', jupyter=True)\n",
    "for token in doc_block[:]:\n",
    "    print(token, token.pos_)\n",
    "nouns_verbs = [token.text for token in doc_block if token.pos_ in ('NOUN', 'VERB')]\n",
    "\n",
    "adj = [token.text for token in doc_block if token.pos_ in ('ADJ')]\n",
    "adj1 = ' '.join([str(elem) for elem in adj])\n",
    "wordcloud = WordCloud(width = 3000,\n",
    "                      height = 1500,\n",
    "                      background_color='black').generate(adj1)\n",
    "# Plot\n",
    "plot_cloud(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cD4iQWsgKyBd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "cD4iQWsgKyBd",
    "outputId": "ca4161a7-85c5-40e5-f412-897fe3e29f43"
   },
   "outputs": [],
   "source": [
    "temp = [y.strip() for y in df3.Product_Description]\n",
    "temp = [x for x in temp if x]\n",
    "text = ' '.join(temp)\n",
    "doc_block = nlp(text)\n",
    "spacy.displacy.render(doc_block, style='ent', jupyter=True)\n",
    "for token in doc_block[:]:\n",
    "    print(token, token.pos_)\n",
    "nouns_verbs = [token.text for token in doc_block if token.pos_ in ('NOUN', 'VERB')]\n",
    "\n",
    "adj = [token.text for token in doc_block if token.pos_ in ('ADJ')]\n",
    "adj1 = ' '.join([str(elem) for elem in adj])\n",
    "wordcloud = WordCloud(width = 3000,\n",
    "                      height = 1500,\n",
    "                      background_color='black').generate(adj1)\n",
    "# Plot\n",
    "plot_cloud(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fvy8bvh-Kx-E",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "fvy8bvh-Kx-E",
    "outputId": "0b54f815-3398-41c1-a428-4dab06fc0358"
   },
   "outputs": [],
   "source": [
    "temp = [y.strip() for y in df4.Product_Description]\n",
    "temp = [x for x in temp if x]\n",
    "text = ' '.join(temp)\n",
    "doc_block = nlp(text)\n",
    "spacy.displacy.render(doc_block, style='ent', jupyter=True)\n",
    "for token in doc_block[:]:\n",
    "    print(token, token.pos_)\n",
    "nouns_verbs = [token.text for token in doc_block if token.pos_ in ('NOUN', 'VERB')]\n",
    "\n",
    "adj = [token.text for token in doc_block if token.pos_ in ('ADJ')]\n",
    "adj1 = ' '.join([str(elem) for elem in adj])\n",
    "wordcloud = WordCloud(width = 3000,\n",
    "                      height = 1500,\n",
    "                      background_color='black').generate(adj1)\n",
    "# Plot\n",
    "plot_cloud(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NyYWIL4MKx6s",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "NyYWIL4MKx6s",
    "outputId": "ba2fe17f-f2ce-4ad6-f7c5-7fcf36bb5993"
   },
   "outputs": [],
   "source": [
    "temp = [y.strip() for y in df5.Product_Description]\n",
    "temp = [x for x in temp if x]\n",
    "text = ' '.join(temp)\n",
    "doc_block = nlp(text)\n",
    "spacy.displacy.render(doc_block, style='ent', jupyter=True)\n",
    "for token in doc_block[:]:\n",
    "    print(token, token.pos_)\n",
    "nouns_verbs = [token.text for token in doc_block if token.pos_ in ('NOUN', 'VERB')]\n",
    "\n",
    "adj = [token.text for token in doc_block if token.pos_ in ('ADJ')]\n",
    "adj1 = ' '.join([str(elem) for elem in adj])\n",
    "wordcloud = WordCloud(width = 3000,\n",
    "                      height = 1500,\n",
    "                      background_color='black').generate(adj1)\n",
    "# Plot\n",
    "plot_cloud(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kp4OWUABKx3D",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "kp4OWUABKx3D",
    "outputId": "fa753965-a81e-48a8-d474-88ff75c6c03a"
   },
   "outputs": [],
   "source": [
    "temp = [y.strip() for y in df6.Product_Description]\n",
    "temp = [x for x in temp if x]\n",
    "text = ' '.join(temp)\n",
    "doc_block = nlp(text)\n",
    "spacy.displacy.render(doc_block, style='ent', jupyter=True)\n",
    "for token in doc_block[:]:\n",
    "    print(token, token.pos_)\n",
    "nouns_verbs = [token.text for token in doc_block if token.pos_ in ('NOUN', 'VERB')]\n",
    "\n",
    "adj = [token.text for token in doc_block if token.pos_ in ('ADJ')]\n",
    "adj1 = ' '.join([str(elem) for elem in adj])\n",
    "wordcloud = WordCloud(width = 3000,\n",
    "                      height = 1500,\n",
    "                      background_color='black').generate(adj1)\n",
    "# Plot\n",
    "plot_cloud(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DF0mZBvKKxx5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "DF0mZBvKKxx5",
    "outputId": "51173ca8-eabc-410f-ebfd-e78e362d92f0"
   },
   "outputs": [],
   "source": [
    "temp = [y.strip() for y in df7.Product_Description]\n",
    "temp = [x for x in temp if x]\n",
    "text = ' '.join(temp)\n",
    "doc_block = nlp(text)\n",
    "spacy.displacy.render(doc_block, style='ent', jupyter=True)\n",
    "for token in doc_block[:]:\n",
    "    print(token, token.pos_)\n",
    "nouns_verbs = [token.text for token in doc_block if token.pos_ in ('NOUN', 'VERB')]\n",
    "\n",
    "adj = [token.text for token in doc_block if token.pos_ in ('ADJ')]\n",
    "adj1 = ' '.join([str(elem) for elem in adj])\n",
    "wordcloud = WordCloud(width = 3000,\n",
    "                      height = 1500,\n",
    "                      background_color='black').generate(adj1)\n",
    "# Plot\n",
    "plot_cloud(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QnaCIxiGRPuO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "QnaCIxiGRPuO",
    "outputId": "e50ef506-ffc5-4ccf-c35e-2b1a204d308c"
   },
   "outputs": [],
   "source": [
    "temp = [y.strip() for y in df8.Product_Description]\n",
    "temp = [x for x in temp if x]\n",
    "text = ' '.join(temp)\n",
    "doc_block = nlp(text)\n",
    "spacy.displacy.render(doc_block, style='ent', jupyter=True)\n",
    "for token in doc_block[:]:\n",
    "    print(token, token.pos_)\n",
    "nouns_verbs = [token.text for token in doc_block if token.pos_ in ('NOUN', 'VERB')]\n",
    "\n",
    "adj = [token.text for token in doc_block if token.pos_ in ('ADJ')]\n",
    "adj1 = ' '.join([str(elem) for elem in adj])\n",
    "wordcloud = WordCloud(width = 3000,\n",
    "                      height = 1500,\n",
    "                      background_color='black').generate(adj1)\n",
    "# Plot\n",
    "plot_cloud(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raty3_VVRPov",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "raty3_VVRPov",
    "outputId": "d17f3ef2-7c3b-46b2-b5a8-c7545d724d47"
   },
   "outputs": [],
   "source": [
    "temp = [y.strip() for y in df9.Product_Description]\n",
    "temp = [x for x in temp if x]\n",
    "text = ' '.join(temp)\n",
    "doc_block = nlp(text)\n",
    "spacy.displacy.render(doc_block, style='ent', jupyter=True)\n",
    "for token in doc_block[:]:\n",
    "    print(token, token.pos_)\n",
    "nouns_verbs = [token.text for token in doc_block if token.pos_ in ('NOUN', 'VERB')]\n",
    "\n",
    "adj = [token.text for token in doc_block if token.pos_ in ('ADJ')]\n",
    "adj1 = ' '.join([str(elem) for elem in adj])\n",
    "wordcloud = WordCloud(width = 3000,\n",
    "                      height = 1500,\n",
    "                      background_color='black').generate(adj1)\n",
    "# Plot\n",
    "plot_cloud(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t4Xnx0O8RPds",
   "metadata": {
    "id": "t4Xnx0O8RPds"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "#Tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "text_tokens = word_tokenize(no_punc_text)\n",
    "text_tokens\n",
    "\n",
    "#Remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend([\"mention\",\"sxsw\",\"SXSW\",\"RT\",\"link\",\"Austin\",\"amp\",\"via\",\"be\",\"4\",\"sxswi\",\"w\",\"re\",\"if\",\"3\",\"5\",\"a\",\"u\",\"SxSW\",\"s\",\"re\",\"m\",\"SXSWi\",\"\\x89ÛÏmention\",\"pm\",\"fb\",\"ll\",\"sxsw\\x89Û\\x9d\",\"2\",\"qagb\",\"Sxsw\",\"\\x89ûïmention\",\"I\"])\n",
    "\n",
    "reviews = [word for word in text_tokens if not word in stop_words]\n",
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57b47bf-aed6-4abd-9ee2-e4ff2d4d530d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Product_details.csv\")\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e367d46-b62c-480b-a338-699d60d6eb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fb69c5-74a5-46cc-bca9-c095ca713572",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Product_Type\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272fc19a-908d-48ce-84a7-2ef570d51d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Sentiment\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7a8cd3-47dc-45ee-b1be-a4df93f9ddd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sentiment_unique = len(data[\"Sentiment\"].unique())\n",
    "print(\"Number of Unique Sentiment: \" + str(Sentiment_unique))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ad833c-8fc0-4a03-9940-d035bbc962bd",
   "metadata": {},
   "source": [
    "## Visualizing the distributions of numerical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d62ead1-94d9-4444-81fa-dde1ad38f098",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.hist(bins=50, figsize=(20,15)) # builds histogram and set the number of bins and fig size (width, height)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c97190a-deda-4c23-b9ae-50559e83e259",
   "metadata": {},
   "source": [
    "# Split into Train/Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00c20d4-4952-447b-b32c-c8dfe5fdcdce",
   "metadata": {},
   "source": [
    "- Before we explore the dataset we're going to split it into training set and test sets\n",
    "- Our goal is to eventually train a sentiment analysis classifier\n",
    "- Since the majority of reviews are positive (5 stars), we will need to do a stratified split on the reviews score to ensure that we don't train the classifier on imbalanced data\n",
    "- To use sklearn's `Stratified ShuffleSplit` class, we're going to remove all samples that have NAN in review score, then covert all review scores to `integer` datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd1f521-e8c9-4fe9-8764-a4dff12473ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "print(\"Before {}\".format(len(data)))\n",
    "dataAfter = data.dropna(subset=[\"Sentiment\"]) # removes all NAN in reviews.rating\n",
    "print(\"After {}\".format(len(dataAfter)))\n",
    "dataAfter[\"Sentiment\"] = dataAfter[\"Sentiment\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1819ace-14de-4a8e-8d6f-4c591ec6a48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = StratifiedShuffleSplit(n_splits=5, test_size=0.2)\n",
    "for train_index, test_index in split.split(dataAfter, dataAfter[\"Sentiment\"]): \n",
    "    strat_train = dataAfter.reindex(train_index)\n",
    "    strat_test = dataAfter.reindex(test_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c76dbd7-3467-4f65-b7f0-29b625ad902f",
   "metadata": {},
   "source": [
    "### Check to see if train/test sets were stratified proportionately in comparison to raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f82a38-743a-4a3b-a6ff-22d41ec0dd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(strat_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f953f21a-adc2-4c91-b094-19ef1f9fd951",
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_train[\"Sentiment\"].value_counts()/len(strat_train) # value_count() counts all the values based on column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae309945-ffaf-487c-957a-b5c255c10117",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(strat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d064a527-97ff-4c54-9cf8-61b2aa29ef2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_test[\"Sentiment\"].value_counts()/len(strat_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56aa3fdb-bd83-4e44-88ae-b9c2c2dbdfd7",
   "metadata": {},
   "source": [
    "##  Data Exploration (Training Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8a5a4f-33d0-4a81-b74d-6fc97faf9c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = strat_train.copy()\n",
    "reviews.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389d3e7b-f324-4453-83e8-5637376f208c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reviews[\"Product_Description\"].unique()), len(reviews[\"Product_Type\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1807cdca-643e-4cf1-a5ed-4295315858b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.groupby(\"Product_Description\")[\"Product_Type\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c47579-b3c1-4695-bf64-bb428db8382d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see all the different text_id's for same product type \n",
    "different_names = reviews[reviews[\"Product_Type\"] == 0][\"Product_Description\"].unique()\n",
    "for i in different_names:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aff1cac-9d40-4e53-8cd4-4bd9b1ad09cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[reviews[\"Product_Type\"] == 1][\"Product_Description\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f1d3f9-7c3b-4c8d-9d87-6ccc2f1c5221",
   "metadata": {},
   "source": [
    "### Confirmed our hypothesis that each product_id can have multiple Text_ID . Therefore we should only really concern ourselves with which product_id's do well, not the Text_ID's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a503a2-8dd0-432c-9f70-6f9a96b91854",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,8))\n",
    "ax1 = plt.subplot(211)\n",
    "ax2 = plt.subplot(212, sharex = ax1)\n",
    "reviews[\"Product_Type\"].value_counts().plot(kind=\"bar\", ax=ax1, title=\"Product_Type Frequency\")\n",
    "np.log10(reviews[\"Product_Type\"].value_counts()).plot(kind=\"bar\", ax=ax2, title=\"Product_Type Frequency (Log10 Adjusted)\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8492d028-9231-4f8c-aabf-b5af527fd3f8",
   "metadata": {},
   "source": [
    "- Based on the bar graph for Product_type, we see that certain products have significantly more reviews than other products.\n",
    "- We also took the log of the Product_type to normalize the data, in order display an in-depth picture of each Product_type, and we see that the distribution still follows a \"right tailed\" distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6fe8e5-f609-4ce3-b1b6-426744411f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entire training dataset average rating\n",
    "reviews[\"Sentiment\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d616841-a761-42b5-8fc5-4a9bad732038",
   "metadata": {},
   "source": [
    "## Sentiment /Product_type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df468ff-a428-44fe-a945-f939e7949f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "Product_type_count_ix = reviews[\"Product_Type\"].value_counts().index\n",
    "plt.subplots(2,1,figsize=(16,8))\n",
    "plt.subplot(2,1,1)\n",
    "reviews[\"Product_Type\"].value_counts().plot(kind=\"bar\", title=\"Product_Type Frequency\")\n",
    "plt.subplot(2,1,2)\n",
    "sns.pointplot(x=\"Product_Type\", y=\"Sentiment\", order=Product_type_count_ix, data=reviews)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ee7dc4-42ea-4964-a3ee-9bbe4c55cb1c",
   "metadata": {},
   "source": [
    "- The most frequently sentiment product_type have their average sentiments in the 2-3 range, with little variance\n",
    "- Although there is a slight inverse relationship between the product_type frequency level and average sentiment for the 8th product_type.\n",
    "**<u>Note</u> that point-plot graph automatically takes the average of the sentiment data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8cbb31-b907-4855-b6c8-5c2ba002604e",
   "metadata": {},
   "source": [
    "# Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7fd0bb-6d43-4aee-9e6f-e4496e8575e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we can analyze reviews.ratings with asins\n",
    "corr_matrix = reviews.corr()\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fa3881-97ac-4a5b-a6c2-5460acab0108",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = reviews[\"Product_Type\"].value_counts().to_frame()\n",
    "counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3aca49-78ed-4e21-8b29-2a2395401efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_rating = reviews.groupby(\"Product_Type\")[\"Sentiment\"].mean().to_frame()\n",
    "avg_rating.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f60e54-dbcb-481d-a405-3e96869db4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = counts.join(avg_rating)\n",
    "table.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452f56a8-5e5a-46f4-ba30-2dfedde7977d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\"Product_Type\", \"Sentiment\", data=table)\n",
    "table.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28898af0-225e-4929-8e2a-a037175fa34e",
   "metadata": {},
   "source": [
    "From our analysis in data exploration above between Product_type and sentiment, we discovered that there are many Product_type with low occurrence that have high variances, as a result we concluded that theses low occurrence Product_types are not significant in our analysis given the low sample size.\n",
    "<br>\n",
    "<br>\n",
    "Similarly in our correlation analysis between Product_type and sentiments, we see that there is almost negative correlation which is consistent with our findings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e360bce-2c55-4cbf-826a-8b9dbf9e641a",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe85bde9-a642-4304-ba28-4569f6bb5ecd",
   "metadata": {},
   "source": [
    "Using the features in place, we will build a classifier that can determine a Product_description sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d806b5-7012-4dff-8ed5-8be7335a88b5",
   "metadata": {},
   "source": [
    "## Set Target Variable (Sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443b4848-434c-412f-acf6-aaec1219073d",
   "metadata": {},
   "source": [
    "Segregate ratings from 0-3 into positive, neutral, and negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cba6965-4561-46b7-aad5-e5207b488559",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiments(Sentiment):\n",
    "    if (Sentiment == 2) or (Sentiment == 3):\n",
    "        return \"Positive\"\n",
    "    elif (Sentiment == 0):\n",
    "        return \"Neutral\"\n",
    "    elif (Sentiment == 1):\n",
    "        return \"Negative\"\n",
    "# Add sentiments to the data\n",
    "strat_train[\"Sentiment\"] = strat_train[\"Sentiment\"].apply(sentiments)\n",
    "strat_test[\"Sentiment\"] = strat_test[\"Sentiment\"].apply(sentiments)\n",
    "strat_train[\"Sentiment\"][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b96149-51aa-4d98-9acb-53ebb3b56b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X_train = strat_train[\"Product_Description\"]\n",
    "X_train_targetSentiment = strat_train[\"Sentiment\"]\n",
    "X_test = strat_test[\"Product_Description\"]\n",
    "X_test_targetSentiment = strat_test[\"Sentiment\"]\n",
    "print(len(X_train), len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485c2268-3cec-454e-bca2-c346c3c7c6c0",
   "metadata": {},
   "source": [
    "5091  training samples and 1273 testing samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e58f95a-2339-491f-ba43-d9a8ce97214a",
   "metadata": {},
   "source": [
    "## Extract Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1104b432-17c8-49f7-ac35-74e43a20010a",
   "metadata": {},
   "source": [
    "Here we will turn content into numerical feature vectors using the **Bag of Words** strategy:\n",
    "- Assign fixed integer id to each word occurrence (integer indices to word occurrence dictionary)\n",
    "- X[i,j] where i is the integer indices, j is the word occurrence, and X is an array of words (our training set)\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "In order to implement the **Bag of Words** strategy, we will use SciKit-Learn's **CountVectorizer** to performs the following:\n",
    "- Text preprocessing:\n",
    "    - Tokenization (breaking sentences into words)\n",
    "    - Stopwords (filtering \"the\", \"are\", etc)\n",
    "- Occurrence counting (builds a dictionary of features from integer indices with word occurrences)\n",
    "- Feature Vector (converts the dictionary of text documents into a feature vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911c34a7-1b1e-415a-be21-249f845db7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace \"nan\" with space\n",
    "X_train = X_train.fillna(' ')\n",
    "X_test = X_test.fillna(' ')\n",
    "X_train_targetSentiment = X_train_targetSentiment.fillna(' ')\n",
    "X_test_targetSentiment = X_test_targetSentiment.fillna(' ')\n",
    "\n",
    "\n",
    "# Text preprocessing and occurance counting\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train) \n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd4045f-2ee6-4c1d-9e2e-635caf0a449e",
   "metadata": {},
   "source": [
    "Here we have 5091 training samples and 7408 distinct words in our training sample.\n",
    "\n",
    "\n",
    "Also, with longer documents, we typically see higher average count values on words that carry very little meaning, this will overshadow shorter documents that have lower average counts with same frequencies, as a result, we will use **TfidfTransformer** to reduce this redundancy:\n",
    "- Term Frequencies (**Tf**) divides number of occurrences for each word by total number of words\n",
    "- Term Frequencies times Inverse Document Frequency (**Tfidf**) downscales the weights of each word (assigns less value to unimportant stop words ie. \"the\", \"are\", etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935b9361-4886-4d1e-b5ea-4a7e3b5fa063",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3107357-e623-4469-92b2-0938fb39ebf0",
   "metadata": {},
   "source": [
    "## Building a Pipeline from the Extracted Features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab8be6a-642e-4e52-846b-ca38344e7ce1",
   "metadata": {},
   "source": [
    "We will use **Multinominal Naive Bayes** as our Classifier\n",
    "- Multinominal Niave Bayes is most suitable for word counts where data are typically represented as **word vector counts** (number of times outcome number X[i,j] is observed over the n trials), while also ignoring non-occurrences of a feature i\n",
    "- Naive Bayes is a simplified version of Bayes Theorem, where all features are assumed conditioned independent to each other (the classifiers), P(x|y) where x is the feature and y is the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c64d508-4c1c-4f91-af01-4a8209a5797a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "clf_multiNB_pipe = Pipeline([(\"vect\", CountVectorizer()), (\"tfidf\", TfidfTransformer()), (\"clf_nominalNB\", MultinomialNB())])\n",
    "clf_multiNB_pipe.fit(X_train, X_train_targetSentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b594ff3-8e25-4569-b202-e4c1ca8efd35",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4064b46-373f-45ec-9471-d1d3f502f481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "predictedMultiNB = clf_multiNB_pipe.predict(X_test)\n",
    "np.mean(predictedMultiNB == X_test_targetSentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11f6421-8c17-40bf-8c0b-956a157238f2",
   "metadata": {},
   "source": [
    "Here we see that our Multinominal Naive Bayes Classifier has a 91.98% accuracy level based on the features. \n",
    "<br>\n",
    "<br>\n",
    "Next we will conduct the following:\n",
    "- Test other models\n",
    "- Fine tune the best models to avoid over-fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcf671a-192b-45b7-bbbf-e5ac384bc71c",
   "metadata": {},
   "source": [
    "## Testing Other Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ca6cf0-a701-4ca8-91ce-f6358e803678",
   "metadata": {},
   "source": [
    "**Logistic Regression Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef9188f-79a0-47a2-9381-bfbb4e20e0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "clf_logReg_pipe = Pipeline([(\"vect\", CountVectorizer()), (\"tfidf\", TfidfTransformer()), (\"clf_logReg\", LogisticRegression())])\n",
    "clf_logReg_pipe.fit(X_train, X_train_targetSentiment)\n",
    "\n",
    "import numpy as np\n",
    "predictedLogReg = clf_logReg_pipe.predict(X_test)\n",
    "np.mean(predictedLogReg == X_test_targetSentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f71270-2456-421c-ba9b-2057544ff748",
   "metadata": {},
   "source": [
    "**Support Vector Machine Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29afaada-6461-42f2-9f0a-b448ce019dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "clf_linearSVC_pipe = Pipeline([(\"vect\", CountVectorizer()), (\"tfidf\", TfidfTransformer()), (\"clf_linearSVC\", LinearSVC())])\n",
    "clf_linearSVC_pipe.fit(X_train, X_train_targetSentiment)\n",
    "\n",
    "predictedLinearSVC = clf_linearSVC_pipe.predict(X_test)\n",
    "np.mean(predictedLinearSVC == X_test_targetSentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32287294-98d1-46c4-9e93-fba21ee21da5",
   "metadata": {},
   "source": [
    "**Decision Tree Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8104914-7db0-4e19-8708-119c05472405",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf_decisionTree_pipe = Pipeline([(\"vect\", CountVectorizer()), (\"tfidf\", TfidfTransformer()), \n",
    "                                  (\"clf_decisionTree\", DecisionTreeClassifier())])\n",
    "clf_decisionTree_pipe.fit(X_train, X_train_targetSentiment)\n",
    "\n",
    "predictedDecisionTree = clf_decisionTree_pipe.predict(X_test)\n",
    "np.mean(predictedDecisionTree == X_test_targetSentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3316a6-3891-4988-baa0-de3591ac1c1a",
   "metadata": {},
   "source": [
    "**Random Forest Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a95580-f730-4412-9dad-754540be21d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf_randomForest_pipe = Pipeline([(\"vect\", CountVectorizer()), (\"tfidf\", TfidfTransformer()), (\"clf_randomForest\", RandomForestClassifier())])\n",
    "clf_randomForest_pipe.fit(X_train, X_train_targetSentiment)\n",
    "\n",
    "predictedRandomForest = clf_randomForest_pipe.predict(X_test)\n",
    "np.mean(predictedRandomForest == X_test_targetSentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a929ba7-84ec-4e3a-bf57-3d0b14eab0b1",
   "metadata": {},
   "source": [
    "Looks like all the models performed very well (>85%), and we will use the **Support Vector Machine Classifier** since it has the highest accuracy level at **92.30%**.\n",
    "<br>\n",
    "Now we will fine tune the Support Vector Machine model (Linear_SVC) to avoid any potential over-fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ad0a3c-2c97-4d5c-b6b6-4b5699b5728e",
   "metadata": {},
   "source": [
    "## Fine tuning the Support Vector Machine Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b94a59-e305-4b0c-a711-c72ece99a502",
   "metadata": {},
   "source": [
    "- Here we will run a **Grid Search** of the best parameters on a grid of possible values, instead of tweaking the parameters of various components of the chain (ie. use_idf in tfidftransformer)\n",
    "- We will also run the grid search with LinearSVC classifier pipeline, parameters and cpu core maximization\n",
    "- Then we will fit the grid search to our training data set\n",
    "- Next we will use our final classifier (after fine-tuning) to test some arbitrary reviews\n",
    "- Finally we will test the accuracy of our final classifier (after fine-tuning)\n",
    "\n",
    "Note that **Support Vector Machines** is very suitable for classification by measuring extreme values between classes, to differentiate the worst case scenarios so that it can classify between Positive, Neutral and Negative correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774bfee6-433a-462e-90f6-2f84e1e96b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],    \n",
    "             'tfidf__use_idf': (True, False), \n",
    "             } \n",
    "gs_clf_LinearSVC_pipe = GridSearchCV(clf_linearSVC_pipe, parameters, n_jobs=-1)\n",
    "gs_clf_LinearSVC_pipe = gs_clf_LinearSVC_pipe.fit(X_train, X_train_targetSentiment)\n",
    "#new_text = [\"The tablet is good, really liked it.\", # positive\n",
    "         #   \"The tablet is ok, but it works fine.\", # neutral\n",
    "          #  \"The tablet is not good, does not work very well.\"] # negative\n",
    "\n",
    "#X_train_targetSentiment[gs_clf_LinearSVC_pipe.predict(new_text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26654d35-9c1c-41e0-bbbc-4c0280b1fc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedGS_clf_LinearSVC_pipe = gs_clf_LinearSVC_pipe.predict(X_test)\n",
    "np.mean(predictedGS_clf_LinearSVC_pipe == X_test_targetSentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2c1586-c295-4d93-88ea-64808b8ccc2f",
   "metadata": {},
   "source": [
    "**Results:**\n",
    "- After testing some arbitrary reviews, it seems that our features is performing correctly with Positive, Neutral, Negative results\n",
    "- We also see that after running the grid search, our Support Vector Machine Classifier has improved to **92.77%** accuracy level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e0c616-b3a8-41b4-98ca-69b1c5f08f44",
   "metadata": {},
   "source": [
    "## Detailed Performance Analysis of Support Vector Machine Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf0b00e-9080-48d3-a2db-1e524865cc24",
   "metadata": {},
   "source": [
    "For detailed analysis, we will:\n",
    "- Analyze the best mean score of the grid search (classifier, parameters, CPU core)\n",
    "- Analyze the best estimator\n",
    "- Analyze the best parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e69821f-2edf-4f51-9a20-4a31511592ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for performance_analysis in (gs_clf_LinearSVC_pipe.best_score_, \n",
    "                             gs_clf_LinearSVC_pipe.best_estimator_, \n",
    "                             gs_clf_LinearSVC_pipe.best_params_):\n",
    "        print(performance_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446e8327-bc23-470b-ac73-9956362b76b7",
   "metadata": {},
   "source": [
    "- Here we see that the best mean score of the grid search is 92.41% which is very close to our accuracy level of 92.77%\n",
    "- Our best estimator here is also displayed\n",
    "- Lastly, our best parameters are true for use_idf in tfidf, and ngram_range between 1,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3399ab8a-813a-49d2-b7c6-0698ee675956",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(classification_report(X_test_targetSentiment, predictedGS_clf_LinearSVC_pipe))\n",
    "print('Accuracy: {}'. format(accuracy_score(X_test_targetSentiment, predictedGS_clf_LinearSVC_pipe)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5232b1d-90bd-41a4-bfbd-a3d0ff6efdae",
   "metadata": {},
   "source": [
    "Below is the summary of the classification report:\n",
    "- Precision: determines how many objects selected were correct\n",
    "- Recall: tells you how many of the objects that should have been selected were actually selected\n",
    "- F1 score measures the weights of recall and precision (1 means precision and recall are equally important, 0 otherwise)\n",
    "- Support is the number of occurrences of each class\n",
    "\n",
    "The results in this analysis confirms our previous data exploration analysis, where the data are very skewed to the positive reviews as shown by the lower support counts in the classification report. Also, both neutral and negative reviews has large standard deviation with small frequencies, which we would not consider significant as shown by the lower precision, recall and F1 scores in the classification report.\n",
    "\n",
    "However, despite that Neutral and Negative results are not very strong predictors in this data set, it still shows a 92.77% accuracy level in predicting the sentiment analysis, which we tested and worked very well when inputting arbitrary text (new_text). Therefore, we are comfortable here with the skewed data set. Also, as we continue to input new dataset in the future that is more balanced, this model will then re-adjust to a more balanced classifier which will increase the accuracy level.\n",
    "\n",
    "<u>Note</u>: The first row will be ignored as we previously replaced all NAN with \" \". We tried to remove this row when we first imported the raw data, but Pandas `DataFrame` did not like this row removed when we tried to drop all NAN (before stratifying and splitting the dataset). As a result, replacing the NAN with \" \" was the best workaround and the first row will be ignored in this analysis.\n",
    "\n",
    "Finally, the overall result here explains that the product_description in this dataset are generally positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b61faa-3228-49b2-b3d4-e331826a72b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "metrics.confusion_matrix(X_test_targetSentiment, predictedGS_clf_LinearSVC_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e123fb27-36fe-4e8a-98cf-505486d310b9",
   "metadata": {},
   "source": [
    "<u>Note</u>: \n",
    "By considering only row 1-3 and column 1-3 labeled as negative, neutral and positive, we see that positive sentiment can sometimes be confused for one another with neutral and negative ratings, with scores of 21 and 65 respectively. However, based on the overall number of significant positive sentiment at a score 1166, then confusion score of 21 and 65 for neutral and negative ratings respectively are considered insignificant.\n",
    "\n",
    "Also, this is a result of positively skewed dataset, which is consistent with both our data exploration and sentiment analysis. Therefore, we conclude that the product type in this dataset has generally positive sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f13402",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gist": {
   "data": {
    "description": "Text Mining/Text Mining Assignment(Reviews-Emotion Mining).ipynb",
    "public": true
   },
   "id": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
